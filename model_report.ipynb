{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, PrecisionRecallDisplay, RocCurveDisplay, ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pio.renderers.default = \"browser\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fs_prefix = './'\n",
   "id": "a775222d86a4c7b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "df = pd.read_csv(fs_prefix+'data/last_report/prep_data_target_7.csv')\n",
    "# y = df[['target']].values\n",
    "df_old = pd.read_csv(fs_prefix+'data/existing_model_training.csv')\n",
    "df.set_index(df['datetime'], inplace=True)\n",
    "df_old.set_index(df_old['datetime'], inplace=True)\n",
    "df.rename(columns={'Температура окр. среды(C)': 'Температура окр.среды(C)'}, inplace=True)"
   ],
   "id": "fead35863cd11088",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# required_columns = [\n",
    "#     'Вес на крюке(тс)',\n",
    "#     'Положение крюкоблока(м)',\n",
    "#     'Момент на СВП(кН*м)',\n",
    "#     'Обороты СВП(об/мин)',\n",
    "#     'Расход на входе(л/с)',\n",
    "#     'Температура окр.среды(C)',\n",
    "#     'Глубина инструмента(м)',\n",
    "#     'Нагрузка на долото(тс)',\n",
    "#     'Наработка каната(т*км)',\n",
    "# ]\n",
    "\n",
    "required_columns = [\n",
    "    'Вес на крюке(тс)',\n",
    "    'Положение крюкоблока(м)',\n",
    "    'Момент на СВП(кН*м)',\n",
    "    'Обороты СВП(об/мин)',\n",
    "    'Расход на входе(л/с)',\n",
    "    # 'Температура окр.среды(C)',\n",
    "    # 'Глубина инструмента(м)',\n",
    "    # 'Нагрузка на долото(тс)',\n",
    "    # 'Наработка каната(т*км)',\n",
    "    # 'Давление в манифольде(МПа)'\n",
    "]\n",
    "\n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score,\n",
    "    'Precision': precision_score,\n",
    "    'Recall': recall_score,\n",
    "    'F1': f1_score,\n",
    "    'ROC-AUC': roc_auc_score\n",
    "}"
   ],
   "id": "99005e1305799ae9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "column_labels_index = {\n",
    "            'target': 'binary_target'\n",
    "        }\n",
    "    \n",
    "df.rename(columns=column_labels_index, inplace=True)\n",
    "y_test = df[['binary_target']].values"
   ],
   "id": "4ee972448ffa4506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaler.fit(df_old[required_columns]) \n",
    "scaled_features = scaler.transform(df[required_columns])\n",
    "class_counts = df[['binary_target']].value_counts()\n",
    "scale_pos_weight = class_counts[0] / class_counts[1]\n"
   ],
   "id": "d863197171399580",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "version = f'_6_13'\n",
    "filename = f\"Отчет_{version}.txt\"\n",
    "models = {\n",
    "    # 'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight=\"balanced\"),\n",
    "    # 'SVM': SVC(class_weight=\"balanced\",  random_state=42, probability=True),\n",
    "    'RandomForest': RandomForestClassifier(class_weight=\"balanced_subsample\", random_state=42, n_jobs=-1),\n",
    "    # 'LightGBM': LGBMClassifier(class_weight=\"balanced\", reg_lambda = 0.5, objective='binary', random_state=42, n_jobs = -1),\n",
    "    # 'XGboost' : xgb.XGBClassifier(scale_pos_weight=scale_pos_weight, reg_lambda = 0.5, objective='binary:logistic', random_state=42, n_jobs = -1),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, silent=True, iterations=500, loss_function='Logloss', eval_metric='Recall', early_stopping_rounds=20),\n",
    "    # 'HistGB' : HistGradientBoostingClassifier(n_iter_no_change=3, scoring='roc_auc',class_weight='balanced', random_state=42)\n",
    "}"
   ],
   "id": "43d39d9719d36207",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def confm(y_true, y_pred):\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Setting the attributes\n",
    "    fig, px = plt.subplots(figsize=(7.5, 7.5))\n",
    "    px.matshow(conf_matrix, cmap=plt.cm.YlOrRd, alpha=0.5)\n",
    "    for m in range(conf_matrix.shape[0]):\n",
    "        for n in range(conf_matrix.shape[1]):\n",
    "            px.text(x=m, y=n, s=conf_matrix[m, n], va=\"center\", ha=\"center\", size=\"xx-large\")\n",
    "    \n",
    "    # Sets the labels\n",
    "    plt.xlabel(\"Actuals\", fontsize=16)\n",
    "    plt.ylabel(\"Predictions\", fontsize=16)\n",
    "    plt.title(f\"Confusion Matrix | {model_name}\", fontsize=15)\n",
    "    plt.show()"
   ],
   "id": "f88a8bbe2ec43784",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Модели отдельно",
   "id": "43901f1f8632ac69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_total = dict()\n",
    "metrics_total = dict()\n",
    "test_df = pd.DataFrame(index=models.keys(), columns=metrics.keys())\n",
    "\n",
    "for model_name in models:\n",
    "    X_test = pd.DataFrame(data=scaled_features, columns=df[required_columns].columns)\n",
    "    X_test.columns = [s.replace(\" \", \"_\") for s in X_test.columns.tolist()]\n",
    "    \n",
    "    with open(fs_prefix + f'models/RFECV_{model_name}{version}.pkl', 'rb') as f:\n",
    "        print(f'Opening RFECV_{model_name}{version}.pkl')\n",
    "        selector = pickle.load(f)\n",
    "        print(type(selector))\n",
    "    # \n",
    "    selected_features = X_test.columns[selector.support_]\n",
    "    X_test = X_test[selected_features]\n",
    "\n",
    "    with open(fs_prefix + f'models/{model_name}{version}.pkl', 'rb') as f:\n",
    "        print(f'Opening {model_name}{version}.pkl')\n",
    "        model = pickle.load(f)\n",
    "        # print(model)\n",
    "    \n",
    "    # model = make_pipeline(selector, model)\n",
    "    \n",
    "    log_probs = model.predict_proba(X_test)\n",
    "    print(model.classes_[1])\n",
    "    data = pd.DataFrame({\n",
    "        'y_pred': model.predict(X_test),\n",
    "        'log_probs_0': log_probs[:,0],\n",
    "        'log_probs_1':log_probs[:,1],\n",
    "        'class_pred': np.argmax(log_probs, axis=1)\n",
    "        })\n",
    "    # data = data.join(df['datetime'])\n",
    "    data.set_index(df['datetime'], inplace=True)\n",
    "    data_total.update({model_name: data})\n",
    "    # Convert log probabilities to class predictions\n",
    "    results_test = {}\n",
    "    for metric_name, metric_func in metrics.items():\n",
    "        if metric_name == 'ROC-AUC':\n",
    "            # results[metric_name] = roc_auc_score(y_test, grid_search.best_estimator_.predict_proba(X_test)[:, 1])\n",
    "            # y_pred_proba = log_probs[:, 0]\n",
    "            roc_auc = metric_func(y_test, data['log_probs_1'])\n",
    "            results_test[metric_name] = roc_auc\n",
    "            RocCurveDisplay.from_estimator(model, X_test, y_test)\n",
    "        elif metric_name == 'Accuracy':\n",
    "            results_test[metric_name] = metric_func(y_test, data['y_pred'])\n",
    "        else:\n",
    "            results_test[metric_name] = metric_func(y_test, data['class_pred'], average='weighted')\n",
    "    confm(y_test, data['y_pred'])\n",
    "    test_df.loc[model_name, :] = results_test\n",
    "    print(test_df.loc[model_name, :])\n",
    "    test_df = round(test_df, 4).sort_values(by='ROC-AUC', ascending=False)\n",
    "    with open(filename, 'a+') as file:\n",
    "        file.write(f\"Метрики на тесте по 11 бригаде: \\n{test_df.loc[model_name, :]}\\n\")\n",
    "    \n",
    "    metrics_total.update({model_name: results_test})"
   ],
   "id": "ded6f3096d3b4b3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Комбинированные предикты",
   "id": "8d7827a333e0e85a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: использовать make_pipeline() вместе с селектором при передаче эстиматоров\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df_old.rename(columns=column_labels_index, inplace=True)\n",
    "df_old.columns = [s.replace(\" \", \"_\") for s in df_old.columns.tolist()]\n",
    "\n",
    "X_test = pd.DataFrame(data=scaled_features, columns=df[required_columns].columns)\n",
    "X_test.columns = [s.replace(\" \", \"_\") for s in X_test.columns.tolist()]\n",
    "\n",
    "# required_columns = [s.replace(\" \", \"_\") for s in required_columns]\n",
    "\n",
    "\n",
    "clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\n",
    "            'rf',\n",
    "            make_pipeline(\n",
    "                pickle.load(open(fs_prefix + f'models/RFECV_RandomForest{version}.pkl', 'rb')),\n",
    "                pickle.load(open(fs_prefix + f'models/RandomForest{version}.pkl', 'rb')),\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            'cb',\n",
    "            make_pipeline(\n",
    "                pickle.load(open(fs_prefix + f'models/RFECV_CatBoost{version}.pkl', 'rb')),\n",
    "                pickle.load(open(fs_prefix + f'models/CatBoost{version}.pkl', 'rb')),\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv='prefit'\n",
    ")\n",
    "\n",
    "combined_model = clf.fit(df_old[[s.replace(\" \", \"_\") for s in required_columns]], df_old['binary_target'])"
   ],
   "id": "91a0d9ea2b218dd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "log_probs = combined_model.predict_proba(X_test)\n",
    "print(combined_model.classes_[1])\n",
    "data_combined = pd.DataFrame({\n",
    "    'y_pred': combined_model.predict(X_test),\n",
    "    'log_probs_0': log_probs[:,0],\n",
    "    'log_probs_1':log_probs[:,1],\n",
    "    'class_pred': np.argmax(log_probs, axis=1)\n",
    "})\n",
    "    \n",
    "results_test = {}\n",
    "for metric_name, metric_func in metrics.items():\n",
    "    if metric_name == 'ROC-AUC':\n",
    "        # results[metric_name] = roc_auc_score(y_test, grid_search.best_estimator_.predict_proba(X_test)[:, 1])\n",
    "        # y_pred_proba = log_probs[:, 0]\n",
    "        roc_auc = metric_func(y_test, data_combined['log_probs_1'])\n",
    "        results_test[metric_name] = roc_auc\n",
    "        RocCurveDisplay.from_estimator(combined_model, X_test, y_test)\n",
    "    elif metric_name == 'Accuracy':\n",
    "        results_test[metric_name] = metric_func(y_test, data_combined['y_pred'])\n",
    "    else:\n",
    "        results_test[metric_name] = metric_func(y_test, data_combined['class_pred'], average='weighted')\n",
    "print(results_test)\n",
    "confm(y_test, data_combined['y_pred'])"
   ],
   "id": "291138becd144f00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: завернуть в циклы. \n",
    "\n",
    "y_pred_array = np.array([data_total['RandomForest']['y_pred'], data_total['CatBoost']['y_pred']])\n",
    "class_pred_array = np.array([data_total['RandomForest']['class_pred'], data_total['CatBoost']['class_pred']])\n",
    "log_probs_0_array = np.array([data_total['RandomForest']['log_probs_0'], data_total['CatBoost']['log_probs_0']])\n",
    "log_probs_1_array = np.array([data_total['RandomForest']['log_probs_1'], data_total['CatBoost']['log_probs_1']])\n",
    "\n",
    "y_pred_array = np.mean(y_pred_array, axis=0)\n",
    "class_pred_array = np.mean(class_pred_array, axis=0)\n",
    "log_probs_0_array = np.mean(log_probs_0_array, axis=0)\n",
    "log_probs_1_array = np.mean(log_probs_1_array, axis=0)\n",
    "\n",
    "y_pred_array = np.round(y_pred_array)\n",
    "class_pred_array = np.round(class_pred_array)\n",
    "\n",
    "data_mean = pd.DataFrame({\n",
    "    'y_pred': y_pred_array,\n",
    "    'log_probs_0': log_probs_0_array,\n",
    "    'log_probs_1': log_probs_1_array,\n",
    "    'class_pred': class_pred_array\n",
    "})\n",
    "\n",
    "results_test = {}\n",
    "for metric_name, metric_func in metrics.items():\n",
    "        if metric_name == 'ROC-AUC':\n",
    "            # results[metric_name] = roc_auc_score(y_test, grid_search.best_estimator_.predict_proba(X_test)[:, 1])\n",
    "            # y_pred_proba = log_probs[:, 0]\n",
    "            roc_auc = metric_func(y_test, log_probs_1_array)\n",
    "            results_test[metric_name] = roc_auc\n",
    "            RocCurveDisplay.from_predictions(y_test, log_probs_1_array)\n",
    "        elif metric_name == 'Accuracy':\n",
    "            results_test[metric_name] = metric_func(y_test, y_pred_array)\n",
    "        else:\n",
    "            results_test[metric_name] = metric_func(y_test, class_pred_array, average='weighted')\n",
    "print(results_test)\n",
    "confm(y_test, y_pred_array)\n",
    "            "
   ],
   "id": "ccbb652971d19ecd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from plotly.subplots import make_subplots\n",
    "fig = make_subplots(\n",
    "    rows=len(data_total)+3,\n",
    "    cols=1,\n",
    "    subplot_titles=['Факторы'] + list(data_total.keys()) + ['Combined', 'Mean'],\n",
    "    shared_xaxes=True\n",
    "    )\n",
    "for coln in required_columns:\n",
    "    print(df.head())\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df[coln],\n",
    "            mode='lines',\n",
    "            name=coln\n",
    "        ),\n",
    "        col=1,\n",
    "        row=1\n",
    "    )\n",
    "i = 2\n",
    "\n",
    "for data_name, data_i in data_total.items():\n",
    "    # for coln in data_i.columns:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data_i.index,\n",
    "            y=data_i['y_pred'],\n",
    "            mode='lines',\n",
    "            name='y_pred'\n",
    "        ),\n",
    "        col=1,\n",
    "        row=i\n",
    "    )\n",
    "\n",
    "    i += 1\n",
    "\n",
    "fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data_combined.index,\n",
    "            y=data_combined['y_pred'],\n",
    "            mode='lines',\n",
    "            name='y_pred'\n",
    "        ),\n",
    "        col=1,\n",
    "        row=i\n",
    "    )\n",
    "i += 1\n",
    "\n",
    "fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df.index,\n",
    "            y=data_mean['y_pred'],\n",
    "            mode='lines',\n",
    "            name='y_pred'\n",
    "        ),\n",
    "        col=1,\n",
    "        row=i\n",
    "    )\n",
    "i += 1\n",
    "\n",
    "fig.show()"
   ],
   "id": "fd702cebfa957165",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b449eaea6aa1fb44",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
